{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T08:47:01.478160Z",
     "start_time": "2020-09-27T08:46:54.355278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\envs\\chattel\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T08:47:01.538077Z",
     "start_time": "2020-09-27T08:47:01.481113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-08-12</td>\n",
       "      <td>242.212219</td>\n",
       "      <td>247.622620</td>\n",
       "      <td>241.496490</td>\n",
       "      <td>246.251251</td>\n",
       "      <td>246.251251</td>\n",
       "      <td>4404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-08-13</td>\n",
       "      <td>244.744751</td>\n",
       "      <td>245.840836</td>\n",
       "      <td>243.248245</td>\n",
       "      <td>243.418411</td>\n",
       "      <td>243.418411</td>\n",
       "      <td>3865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-08-16</td>\n",
       "      <td>242.082077</td>\n",
       "      <td>245.180176</td>\n",
       "      <td>240.490494</td>\n",
       "      <td>243.038040</td>\n",
       "      <td>243.038040</td>\n",
       "      <td>2607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-08-17</td>\n",
       "      <td>244.509506</td>\n",
       "      <td>247.597595</td>\n",
       "      <td>243.258255</td>\n",
       "      <td>245.505508</td>\n",
       "      <td>245.505508</td>\n",
       "      <td>3777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-08-18</td>\n",
       "      <td>245.465469</td>\n",
       "      <td>245.680679</td>\n",
       "      <td>241.016022</td>\n",
       "      <td>241.316315</td>\n",
       "      <td>241.316315</td>\n",
       "      <td>5367400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2010-08-12  242.212219  247.622620  241.496490  246.251251  246.251251   \n",
       "1  2010-08-13  244.744751  245.840836  243.248245  243.418411  243.418411   \n",
       "2  2010-08-16  242.082077  245.180176  240.490494  243.038040  243.038040   \n",
       "3  2010-08-17  244.509506  247.597595  243.258255  245.505508  245.505508   \n",
       "4  2010-08-18  245.465469  245.680679  241.016022  241.316315  241.316315   \n",
       "\n",
       "    Volume  \n",
       "0  4404700  \n",
       "1  3865500  \n",
       "2  2607300  \n",
       "3  3777600  \n",
       "4  5367400  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/GOOG.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T08:47:01.695260Z",
     "start_time": "2020-09-27T08:47:01.544051Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, name, input_size, output_size, size_layer):\n",
    "        with tf.variable_scope(name):\n",
    "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
    "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer))\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell,\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    initial_state=self.hidden_layer)\n",
    "            tensor_action, tensor_validation = tf.split(self.rnn[:,-1],2,1)\n",
    "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
    "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
    "            self.logits = feed_validation + tf.subtract(feed_action,\n",
    "                                                        tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, name, input_size, output_size, size_layer, learning_rate):\n",
    "        with tf.variable_scope(name):\n",
    "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
    "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer))\n",
    "            self.REWARD = tf.placeholder(tf.float32, (None, 1))\n",
    "            feed_critic = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell,\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    initial_state=self.hidden_layer)\n",
    "            tensor_action, tensor_validation = tf.split(self.rnn[:,-1],2,1)\n",
    "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
    "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
    "            feed_critic = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
    "            feed_critic = tf.nn.relu(feed_critic) + self.Y\n",
    "            feed_critic = tf.layers.dense(feed_critic, size_layer//2, activation = tf.nn.relu)\n",
    "            self.logits = tf.layers.dense(feed_critic, 1)\n",
    "            self.cost = tf.reduce_mean(tf.square(self.REWARD - self.logits))\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "            \n",
    "class Agent:\n",
    "\n",
    "    LEARNING_RATE = 0.001\n",
    "    BATCH_SIZE = 32\n",
    "    LAYER_SIZE = 256\n",
    "    OUTPUT_SIZE = 3\n",
    "    EPSILON = 0.5\n",
    "    DECAY_RATE = 0.005\n",
    "    MIN_EPSILON = 0.1\n",
    "    GAMMA = 0.99\n",
    "    MEMORIES = deque()\n",
    "    MEMORY_SIZE = 300\n",
    "    COPY = 1000\n",
    "    T_COPY = 0\n",
    "\n",
    "    def __init__(self, state_size, window_size, trend, skip):\n",
    "        self.state_size = state_size\n",
    "        self.window_size = window_size\n",
    "        self.half_window = window_size // 2\n",
    "        self.trend = trend\n",
    "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
    "        self.skip = skip\n",
    "        tf.reset_default_graph()\n",
    "        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
    "        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
    "        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
    "        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, \n",
    "                                    self.LAYER_SIZE, self.LEARNING_RATE)\n",
    "        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n",
    "        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n",
    "        grads = zip(self.grad_actor, weights_actor)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _assign(self, from_name, to_name):\n",
    "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
    "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
    "        for i in range(len(from_w)):\n",
    "            assign_op = to_w[i].assign(from_w[i])\n",
    "            self.sess.run(assign_op)\n",
    "            \n",
    "    def _memorize(self, state, action, reward, new_state, dead, rnn_state):\n",
    "        self.MEMORIES.append((state, action, reward, new_state, dead, rnn_state))\n",
    "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
    "            self.MEMORIES.popleft()\n",
    "            \n",
    "    def _select_action(self, state):\n",
    "        if np.random.rand() < self.EPSILON:\n",
    "            action = np.random.randint(self.OUTPUT_SIZE)\n",
    "        else:\n",
    "            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X:[state]})[0]\n",
    "            action = np.argmax(prediction)\n",
    "        return action\n",
    "    \n",
    "    def _construct_memories_and_train(self, replay):\n",
    "        states = np.array([a[0] for a in replay])\n",
    "        new_states = np.array([a[3] for a in replay])\n",
    "        init_values = np.array([a[-1] for a in replay])\n",
    "        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states,\n",
    "                                                       self.actor.hidden_layer: init_values})\n",
    "        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states,\n",
    "                                                                     self.actor_target.hidden_layer: init_values})\n",
    "        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X:states, self.critic.Y:Q,\n",
    "                                                          self.critic.hidden_layer: init_values})[0]\n",
    "        self.sess.run(self.optimizer, feed_dict={self.actor.X:states, self.actor_critic_grad:grads,\n",
    "                                                self.actor.hidden_layer: init_values})\n",
    "        \n",
    "        rewards = np.array([a[2] for a in replay]).reshape((-1, 1))\n",
    "        rewards_target = self.sess.run(self.critic_target.logits, \n",
    "                                       feed_dict={self.critic_target.X:new_states,self.critic_target.Y:Q_target,\n",
    "                                                 self.critic_target.hidden_layer: init_values})\n",
    "        for i in range(len(replay)):\n",
    "            if not replay[0][-2]:\n",
    "                rewards[i] += self.GAMMA * rewards_target[i]\n",
    "        cost, _ = self.sess.run([self.critic.cost, self.critic.optimizer], \n",
    "                                feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.REWARD:rewards,\n",
    "                                          self.critic.hidden_layer: init_values})\n",
    "        return cost\n",
    "    \n",
    "    def get_state(self, t):\n",
    "        window_size = self.window_size + 1\n",
    "        d = t - window_size + 1\n",
    "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
    "        res = []\n",
    "        for i in range(window_size - 1):\n",
    "            res.append(block[i + 1] - block[i])\n",
    "        return np.array(res)\n",
    "    \n",
    "    def buy(self, initial_money,date1,close):\n",
    "        starting_money = initial_money\n",
    "        states_sell = []\n",
    "        states_buy = []\n",
    "        inventory = []\n",
    "        date1 = date1\n",
    "        state = self.get_state(0)\n",
    "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
    "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
    "            self.INITIAL_FEATURES[k,:] = state\n",
    "        for t in range(0, len(self.trend) - 1, self.skip):\n",
    "            \n",
    "            if np.random.rand() < self.EPSILON:\n",
    "                action = np.random.randint(self.OUTPUT_SIZE)\n",
    "            else:\n",
    "                action, last_state = self.sess.run([self.actor.logits,\n",
    "                                                  self.actor.last_state],\n",
    "                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n",
    "                                                             self.actor.hidden_layer:init_value})\n",
    "                action, init_value = np.argmax(action[0]), last_state\n",
    "                    \n",
    "            next_state = self.get_state(t + 1)\n",
    "            \n",
    "            if action == 1 and initial_money >= close[t]:\n",
    "                inventory.append(close[t])\n",
    "                initial_money -= close[t]\n",
    "                states_buy.append(t)\n",
    "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, close[t], initial_money))\n",
    "                df1 = pd.DataFrame({'Date': date1[t+1], 'Close': [close[t+1]],'RESULT': ['Buy'] })\n",
    "                if not os.path.isfile('output/13.double-duel-recurrent-q-learning-agent.csv'):\n",
    "                    df1.to_csv('output/13.double-duel-recurrent-q-learning-agent.csv', index=False)\n",
    "                else:\n",
    "                    df1.to_csv('output/13.double-duel-recurrent-q-learning-agent.csv', index=False, mode='a', header=False)\n",
    "            \n",
    "            elif action == 2 and len(inventory):\n",
    "                bought_price = inventory.pop(0)\n",
    "                initial_money += close[t]\n",
    "                states_sell.append(t)\n",
    "                try:\n",
    "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
    "                except:\n",
    "                    invest = 0\n",
    "                print(\n",
    "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
    "                    % (t, close[t], invest, initial_money)\n",
    "                )\n",
    "                df2 = pd.DataFrame({'Date': date1[t+1], 'Close': [close[t+1]],'RESULT': ['Sell'] })\n",
    "                if not os.path.isfile('output/17.actor-critic-duel-recurrent-agent.csv'):\n",
    "                    df2.to_csv('output/17.actor-critic-duel-recurrent-agent.csv', index=False)\n",
    "                else:\n",
    "                    df2.to_csv('output/17.actor-critic-duel-recurrent-agent.csv', index=False, mode='a', header=False)\n",
    "            else:\n",
    "                print(\n",
    "                    'day %d, hold UNIT at price %f,  total balance %f,'\n",
    "                    % (t+1, close[t+1], initial_money)\n",
    "                )\n",
    "                df3 = pd.DataFrame({'Date': date1[t+1], 'Close': [close[t+1]], 'RESULT': ['Hold']})\n",
    "                if not os.path.isfile('output/17.actor-critic-duel-recurrent-agent.csv'):\n",
    "                    df3.to_csv('output/17.actor-critic-duel-recurrent-agent.csv', index=False)\n",
    "                else:\n",
    "                    df3.to_csv('output/17.actor-critic-duel-recurrent-agent.csv', index=False, mode='a', header=False)\n",
    "            \n",
    "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
    "            self.INITIAL_FEATURES = new_state\n",
    "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
    "        total_gains = initial_money - starting_money\n",
    "        print(\n",
    "                '\\ntotal gained %f, total investment %f %%'\n",
    "                % (initial_money - starting_money, invest)\n",
    "            )\n",
    "    \n",
    "    def train(self, iterations, checkpoint, initial_money):\n",
    "        for i in range(iterations):\n",
    "            total_profit = 0\n",
    "            inventory = []\n",
    "            state = self.get_state(0)\n",
    "            starting_money = initial_money\n",
    "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
    "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
    "                self.INITIAL_FEATURES[k,:] = state\n",
    "            for t in range(0, len(self.trend) - 1, self.skip):\n",
    "                if (self.T_COPY + 1) % self.COPY == 0:\n",
    "                    self._assign('actor-original', 'actor-target')\n",
    "                    self._assign('critic-original', 'critic-target')\n",
    "                    \n",
    "                if np.random.rand() < self.EPSILON:\n",
    "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
    "                else:\n",
    "                    action, last_state = self.sess.run([self.actor.logits,\n",
    "                                                  self.actor.last_state],\n",
    "                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n",
    "                                                             self.actor.hidden_layer:init_value})\n",
    "                    action, init_value = np.argmax(action[0]), last_state\n",
    "                \n",
    "                next_state = self.get_state(t + 1)\n",
    "                \n",
    "                if action == 1 and starting_money >= self.trend[t]:\n",
    "                    inventory.append(self.trend[t])\n",
    "                    starting_money -= self.trend[t]\n",
    "                \n",
    "                elif action == 2 and len(inventory) > 0:\n",
    "                    bought_price = inventory.pop(0)\n",
    "                    total_profit += self.trend[t] - bought_price\n",
    "                    starting_money += self.trend[t]\n",
    "                    \n",
    "                invest = ((starting_money - initial_money) / initial_money)\n",
    "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
    "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
    "                               starting_money < initial_money, init_value[0])\n",
    "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
    "                self.INITIAL_FEATURES = new_state\n",
    "                replay = random.sample(self.MEMORIES, batch_size)\n",
    "                cost = self._construct_memories_and_train(replay)\n",
    "                self.T_COPY += 1\n",
    "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
    "            if (i+1) % checkpoint == 0:\n",
    "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
    "                                                                                  starting_money))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T08:49:34.595786Z",
     "start_time": "2020-09-27T08:47:01.700201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-036b1db5b2d2>:9: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000189D48BA240>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-3-036b1db5b2d2>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From d:\\anaconda\\envs\\chattel\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:966: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From d:\\anaconda\\envs\\chattel\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-3-036b1db5b2d2>:14: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From d:\\anaconda\\envs\\chattel\\lib\\site-packages\\tensorflow\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000189D4A62588>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000189D4A69240>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000189D4C8C320>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "epoch: 1, total rewards: 4204.738581.3, cost: 0.102795, total money: 6492.878535\n"
     ]
    }
   ],
   "source": [
    "close = df.Close.values.tolist()\n",
    "initial_money = 10000\n",
    "window_size = 30\n",
    "skip = 1\n",
    "batch_size = 32\n",
    "agent = Agent(state_size = window_size, \n",
    "              window_size = window_size, \n",
    "              trend = close, \n",
    "              skip = skip)\n",
    "agent.train(iterations = 1, checkpoint = 1, initial_money = initial_money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T08:49:35.693604Z",
     "start_time": "2020-09-27T08:49:34.600771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day 1, hold UNIT at price 243.418411,  total balance 10000.000000,\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/17.actor-critic-duel-recurrent-agent.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a6442a0dc535>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_money\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_money\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdate1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-036b1db5b2d2>\u001b[0m in \u001b[0;36mbuy\u001b[1;34m(self, initial_money, date1, close)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdate1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Close'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RESULT'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Hold'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output/17.actor-critic-duel-recurrent-agent.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                     \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output/17.actor-critic-duel-recurrent-agent.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                     \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output/17.actor-critic-duel-recurrent-agent.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\chattel\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3165\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m         )\n\u001b[1;32m-> 3167\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\chattel\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             )\n\u001b[0;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\chattel\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/17.actor-critic-duel-recurrent-agent.csv'"
     ]
    }
   ],
   "source": [
    "date = df.Date.values.tolist()\n",
    "agent.buy(initial_money = initial_money,date1=date,close=close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T08:49:35.702577Z",
     "start_time": "2020-09-27T08:46:54.375Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,5))\n",
    "plt.plot(close, color='r', lw=2.)\n",
    "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
    "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
    "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
